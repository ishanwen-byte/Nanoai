//! NanoAI åŸºç¡€ä½¿ç”¨ç¤ºä¾‹
//! å±•ç¤ºå¦‚ä½•ä½¿ç”¨ nanoai åº“è¿›è¡Œå„ç§ AI å¯¹è¯æ“ä½œ

use futures::StreamExt;
use nanoai::{Config, LLMClient, Message, Result, message};

#[tokio::main]
async fn main() -> Result<()> {
    // åˆå§‹åŒ–æ—¥å¿—
    env_logger::init();

    // ä».envæ–‡ä»¶è·å–APIå¯†é’¥å’Œé…ç½®
    let (api_key, base_url, model) = if let Ok(key) = dotenv::var("OPENROUTER_API_KEY") {
        let model = dotenv::var("OPENROUTER_MODEL").unwrap_or("openai/gpt-3.5-turbo".to_string());
        (key, Some("https://openrouter.ai/api/v1".to_string()), model)
    } else if let Ok(key) = dotenv::var("OPENAI_API_KEY") {
        (key, None, "gpt-3.5-turbo".to_string())
    } else if let Ok(key) = dotenv::var("API_KEY") {
        (key, None, "gpt-3.5-turbo".to_string())
    } else {
        println!("âŒ é”™è¯¯: æœªæ‰¾åˆ°APIå¯†é’¥");
        println!("\nè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€è®¾ç½®APIå¯†é’¥:");
        println!("\næ–¹å¼1: åˆ›å»º.envæ–‡ä»¶ (æ¨è)");
        println!("   OpenAI: OPENAI_API_KEY=your-openai-key");
        println!("   OpenRouter: OPENROUTER_API_KEY=your-openrouter-key");
        println!("              OPENROUTER_MODEL=your-model-name");
        println!("\næ–¹å¼2: è®¾ç½®ç¯å¢ƒå˜é‡");
        println!("   Windows PowerShell: $env:OPENAI_API_KEY=\"your-api-key\"");
        println!("   Windows CMD: set OPENAI_API_KEY=your-api-key");
        return Ok(());
    };

    println!("âœ… APIå¯†é’¥å·²è®¾ç½®");
    println!("ğŸ”§ ä½¿ç”¨æ¨¡å‹: {model}");

    println!("ğŸš€ NanoAI åŸºç¡€ä½¿ç”¨ç¤ºä¾‹\n");

    // ç¤ºä¾‹1: åŸºç¡€é…ç½®å’Œç®€å•å¯¹è¯
    basic_chat_example(&api_key, &base_url, &model).await?;

    // ç¤ºä¾‹2: è‡ªå®šä¹‰é…ç½®
    custom_config_example(&api_key, &base_url, &model).await?;

    // ç¤ºä¾‹3: å¤šè½®å¯¹è¯
    multi_turn_conversation(&api_key, &base_url, &model).await?;

    // ç¤ºä¾‹4: æµå¼å“åº”
    streaming_example(&api_key, &base_url, &model).await?;

    // ç¤ºä¾‹5: é”™è¯¯å¤„ç†
    error_handling_example(&api_key, &base_url, &model).await?;

    println!("\nâœ… æ‰€æœ‰ç¤ºä¾‹æ‰§è¡Œå®Œæˆï¼");
    Ok(())
}

/// ç¤ºä¾‹1: åŸºç¡€é…ç½®å’Œç®€å•å¯¹è¯
async fn basic_chat_example(api_key: &str, base_url: &Option<String>, model: &str) -> Result<()> {
    println!("ğŸ“ ç¤ºä¾‹1: åŸºç¡€å¯¹è¯");

    // åˆ›å»ºé»˜è®¤é…ç½®
    let mut config = Config::default()
        .with_api_key(api_key.to_string())
        .with_model(model.to_string());

    if let Some(url) = base_url {
        config = config.with_base_url(url.clone());
    }

    // åˆ›å»ºå®¢æˆ·ç«¯
    let client = LLMClient::new(config);

    // ç®€å•å¯¹è¯
    let response = client.generate("ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚").await?;
    println!("ğŸ¤– AIå›å¤: {response}");

    println!("âœ… åŸºç¡€å¯¹è¯ç¤ºä¾‹å®Œæˆ\n");
    Ok(())
}

/// ç¤ºä¾‹2: è‡ªå®šä¹‰é…ç½®
async fn custom_config_example(
    api_key: &str,
    base_url: &Option<String>,
    model: &str,
) -> Result<()> {
    println!("âš™ï¸ ç¤ºä¾‹2: è‡ªå®šä¹‰é…ç½®");

    // åˆ›å»ºè‡ªå®šä¹‰é…ç½®
    let mut config = Config::default()
        .with_api_key(api_key.to_string())
        .with_model(model.to_string())
        .with_temperature(0.9); // æ›´é«˜çš„åˆ›é€ æ€§

    if let Some(url) = base_url {
        config = config.with_base_url(url.clone());
    }

    let client = LLMClient::new(config);

    // åˆ›é€ æ€§å†™ä½œä»»åŠ¡
    let prompt = "å†™ä¸€ä¸ªå…³äºæœºå™¨äººå­¦ä¼šåšé¥­çš„æœ‰è¶£å°æ•…äº‹ï¼Œå¤§çº¦100å­—ã€‚";
    let response = client.generate(prompt).await?;
    println!("ğŸ¤– åˆ›æ„æ•…äº‹: {response}");

    println!("âœ… è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹å®Œæˆ\n");
    Ok(())
}

/// ç¤ºä¾‹3: å¤šè½®å¯¹è¯
async fn multi_turn_conversation(
    api_key: &str,
    base_url: &Option<String>,
    model: &str,
) -> Result<()> {
    println!("ğŸ’¬ ç¤ºä¾‹3: å¤šè½®å¯¹è¯");

    let mut config = Config::default()
        .with_api_key(api_key.to_string())
        .with_model(model.to_string());

    if let Some(url) = base_url {
        config = config.with_base_url(url.clone());
    }

    let client = LLMClient::new(config);

    // æ„å»ºå¯¹è¯å†å²
    let messages = vec![
        message("user", "æˆ‘æƒ³å­¦ä¹  Rust ç¼–ç¨‹è¯­è¨€"),
        message(
            "assistant",
            "å¤ªå¥½äº†ï¼Rust æ˜¯ä¸€é—¨ç³»ç»Ÿç¼–ç¨‹è¯­è¨€ï¼Œä»¥å†…å­˜å®‰å…¨å’Œé«˜æ€§èƒ½è‘—ç§°ã€‚ä½ æƒ³ä»å“ªä¸ªæ–¹é¢å¼€å§‹å­¦ä¹ ï¼Ÿ",
        ),
        message("user", "è¯·æ¨èä¸€äº›é€‚åˆåˆå­¦è€…çš„å­¦ä¹ èµ„æº"),
    ];

    let system_message = "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ç¼–ç¨‹å¯¼å¸ˆï¼Œä¸“é—¨å¸®åŠ©åˆå­¦è€…å­¦ä¹ ç¼–ç¨‹ã€‚";
    let response = client
        .generate_with_context(system_message, &messages)
        .await?;

    println!("ğŸ¤– ç¼–ç¨‹å¯¼å¸ˆå›å¤: {response}");
    println!("âœ… å¤šè½®å¯¹è¯ç¤ºä¾‹å®Œæˆ\n");
    Ok(())
}

/// ç¤ºä¾‹4: æµå¼å“åº”
async fn streaming_example(api_key: &str, base_url: &Option<String>, model: &str) -> Result<()> {
    println!("ğŸŒŠ ç¤ºä¾‹4: æµå¼å“åº”");

    let mut config = Config::default()
        .with_api_key(api_key.to_string())
        .with_model(model.to_string());

    if let Some(url) = base_url {
        config = config.with_base_url(url.clone());
    }

    let client = LLMClient::new(config);

    println!("ğŸ¤– AIæ­£åœ¨æ€è€ƒå¹¶é€æ­¥å›å¤...");
    print!("å›å¤: ");

    // æµå¼ç”Ÿæˆ
    let mut stream = client
        .generate_stream("è¯·è§£é‡Šä»€ä¹ˆæ˜¯å‡½æ•°å¼ç¼–ç¨‹ï¼Œå¹¶ç»™å‡ºä¸€ä¸ªç®€å•çš„ä¾‹å­ã€‚")
        .await?;

    while let Some(chunk_result) = stream.next().await {
        match chunk_result {
            Ok(chunk) => {
                print!("{chunk}");
                // åˆ·æ–°è¾“å‡ºç¼“å†²åŒºä»¥å®æ—¶æ˜¾ç¤º
                use std::io::{self, Write};
                io::stdout().flush().unwrap();
            }
            Err(e) => {
                eprintln!("\nâŒ æµå¼å“åº”é”™è¯¯: {e}");
                break;
            }
        }
    }

    println!("\nâœ… æµå¼å“åº”ç¤ºä¾‹å®Œæˆ\n");
    Ok(())
}

/// ç¤ºä¾‹5: é”™è¯¯å¤„ç†
async fn error_handling_example(
    api_key: &str,
    base_url: &Option<String>,
    model: &str,
) -> Result<()> {
    println!("ğŸ›¡ï¸ ç¤ºä¾‹5: é”™è¯¯å¤„ç†");

    // æ•…æ„ä½¿ç”¨é”™è¯¯çš„é…ç½®æ¥æ¼”ç¤ºé”™è¯¯å¤„ç†
    let mut config = Config::default()
        .with_api_key("invalid_key".to_string()) // æ— æ•ˆçš„APIå¯†é’¥
        .with_model(model.to_string());

    if let Some(url) = base_url {
        config = config.with_base_url(url.clone());
    }

    let client = LLMClient::new(config);

    // å°è¯•è°ƒç”¨APIå¹¶å¤„ç†é”™è¯¯
    match client.generate("Hello").await {
        Ok(response) => {
            println!("ğŸ¤– æ„å¤–æˆåŠŸ: {response}");
        }
        Err(e) => {
            println!("âŒ é¢„æœŸçš„é”™è¯¯: {e}");

            // æ ¹æ®é”™è¯¯ç±»å‹è¿›è¡Œä¸åŒå¤„ç†
            match e {
                nanoai::NanoError::Api(msg) => {
                    println!("   è¿™æ˜¯ä¸€ä¸ªAPIé”™è¯¯: {msg}");
                }
                nanoai::NanoError::Http(_) => {
                    println!("   è¿™æ˜¯ä¸€ä¸ªHTTPé”™è¯¯");
                }
                nanoai::NanoError::Timeout => {
                    println!("   è¯·æ±‚è¶…æ—¶");
                }
                _ => {
                    println!("   å…¶ä»–ç±»å‹çš„é”™è¯¯");
                }
            }
        }
    }

    // ç°åœ¨ä½¿ç”¨æ­£ç¡®çš„é…ç½®
    println!("\nğŸ”§ ä½¿ç”¨æ­£ç¡®çš„é…ç½®é‡è¯•...");
    let mut correct_config = Config::default()
        .with_api_key(api_key.to_string())
        .with_model(model.to_string());

    if let Some(url) = base_url {
        correct_config = correct_config.with_base_url(url.clone());
    }

    let correct_client = LLMClient::new(correct_config);
    let response = correct_client
        .generate("ç®€å•è¯´ä¸€å¥è¯è¯æ˜ä½ æ­£å¸¸å·¥ä½œã€‚")
        .await?;
    println!("ğŸ¤– æ­£å¸¸å›å¤: {response}");

    println!("âœ… é”™è¯¯å¤„ç†ç¤ºä¾‹å®Œæˆ\n");
    Ok(())
}

/// è¾…åŠ©å‡½æ•°ï¼šæ¼”ç¤ºä¸åŒçš„æ¶ˆæ¯åˆ›å»ºæ–¹å¼
#[allow(dead_code)]
fn demonstrate_message_creation() {
    // æ–¹å¼1: ä½¿ç”¨ä¾¿åˆ©å‡½æ•°
    let _msg1 = message("user", "Hello");

    // æ–¹å¼2: ç›´æ¥åˆ›å»ºç»“æ„ä½“
    let _msg2 = Message {
        role: "assistant".to_string(),
        content: "Hi there!".to_string(),
    };

    // æ–¹å¼3: æ‰¹é‡åˆ›å»º
    let _messages = [
        message("system", "You are a helpful assistant."),
        message("user", "What's the weather like?"),
        message(
            "assistant",
            "I don't have access to real-time weather data.",
        ),
    ];
}
