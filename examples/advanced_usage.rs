//! NanoAI é«˜çº§ä½¿ç”¨ç¤ºä¾‹
//! å±•ç¤ºæ›´å¤æ‚çš„ä½¿ç”¨åœºæ™¯ï¼ŒåŒ…æ‹¬å¹¶å‘å¤„ç†ã€æ‰¹é‡æ“ä½œç­‰

use futures::{StreamExt, stream};
use nanoai::{Config, LLMClient, Result, message};
use std::env;
use std::time::{Duration, Instant};
use tokio;
use tokio::time::sleep;

#[tokio::main]
async fn main() -> Result<()> {
    // åˆå§‹åŒ–æ—¥å¿—
    env_logger::init();

    let api_key = env::var("OPENAI_API_KEY")
        .or_else(|_| env::var("API_KEY"))
        .expect("è¯·è®¾ç½® OPENAI_API_KEY æˆ– API_KEY ç¯å¢ƒå˜é‡");

    println!("ğŸš€ NanoAI é«˜çº§ä½¿ç”¨ç¤ºä¾‹\n");

    // ç¤ºä¾‹1: å¹¶å‘å¤„ç†å¤šä¸ªè¯·æ±‚
    concurrent_requests_example(&api_key).await?;

    // ç¤ºä¾‹2: æ‰¹é‡æ–‡æœ¬å¤„ç†
    batch_processing_example(&api_key).await?;

    // ç¤ºä¾‹3: æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ
    intelligent_chat_system(&api_key).await?;

    // ç¤ºä¾‹4: æ€§èƒ½æµ‹è¯•å’Œç›‘æ§
    performance_monitoring_example(&api_key).await?;

    // ç¤ºä¾‹5: ä¸åŒæ¨¡å‹æ¯”è¾ƒ
    model_comparison_example(&api_key).await?;

    println!("\nâœ… æ‰€æœ‰é«˜çº§ç¤ºä¾‹æ‰§è¡Œå®Œæˆï¼");
    Ok(())
}

/// ç¤ºä¾‹1: å¹¶å‘å¤„ç†å¤šä¸ªè¯·æ±‚
async fn concurrent_requests_example(api_key: &str) -> Result<()> {
    println!("âš¡ ç¤ºä¾‹1: å¹¶å‘å¤„ç†å¤šä¸ªè¯·æ±‚");

    let config = Config::default()
        .with_api_key(api_key.to_string())
        .with_model("gpt-3.5-turbo".to_string());

    let client = LLMClient::new(config);

    // å‡†å¤‡å¤šä¸ªä¸åŒçš„é—®é¢˜
    let questions = vec![
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "è§£é‡Šä¸€ä¸‹æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ",
        "æ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ",
        "è‡ªç„¶è¯­è¨€å¤„ç†çš„ä¸»è¦åº”ç”¨æœ‰å“ªäº›ï¼Ÿ",
    ];

    let start_time = Instant::now();

    // å¹¶å‘æ‰§è¡Œæ‰€æœ‰è¯·æ±‚
    let tasks: Vec<_> = questions
        .into_iter()
        .enumerate()
        .map(|(i, question)| {
            let client = client.clone();
            let question = question.to_string();
            tokio::spawn(async move {
                println!("ğŸ”„ å¼€å§‹å¤„ç†é—®é¢˜ {}: {question}", i + 1);
                let result = client.generate(&question).await;
                (i + 1, question, result)
            })
        })
        .collect();

    // ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    for task in tasks {
        match task.await {
            Ok((index, question, result)) => match result {
                Ok(answer) => {
                    println!("âœ… é—®é¢˜ {index}: {question}");
                    println!(
                        "ğŸ¤– å›ç­”: {}\n",
                        answer.chars().take(100).collect::<String>() + "..."
                    );
                }
                Err(e) => {
                    println!("âŒ é—®é¢˜ {index} å¤„ç†å¤±è´¥: {e}\n");
                }
            },
            Err(e) => {
                println!("âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}\n");
            }
        }
    }

    let elapsed = start_time.elapsed();
    println!("â±ï¸ å¹¶å‘å¤„ç†è€—æ—¶: {elapsed:?}");
    println!("âœ… å¹¶å‘è¯·æ±‚ç¤ºä¾‹å®Œæˆ\n");
    Ok(())
}

/// ç¤ºä¾‹2: æ‰¹é‡æ–‡æœ¬å¤„ç†
async fn batch_processing_example(api_key: &str) -> Result<()> {
    println!("ğŸ“¦ ç¤ºä¾‹2: æ‰¹é‡æ–‡æœ¬å¤„ç†");

    let config = Config::default()
        .with_api_key(api_key.to_string())
        .with_model("gpt-3.5-turbo".to_string())
        .with_temperature(0.3); // æ›´ä¸€è‡´çš„è¾“å‡º

    let client = LLMClient::new(config);

    // æ¨¡æ‹Ÿéœ€è¦å¤„ç†çš„æ–‡æœ¬åˆ—è¡¨
    let texts = vec![
        "ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œé˜³å…‰æ˜åªšã€‚",
        "æˆ‘å–œæ¬¢åœ¨å‘¨æœ«è¯»ä¹¦å’Œçœ‹ç”µå½±ã€‚",
        "ç¼–ç¨‹æ˜¯ä¸€é—¨è‰ºæœ¯ï¼Œä¹Ÿæ˜¯ä¸€é—¨ç§‘å­¦ã€‚",
        "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ä¸–ç•Œã€‚",
        "å­¦ä¹ æ–°æŠ€èƒ½éœ€è¦è€å¿ƒå’ŒåšæŒã€‚",
    ];

    println!("ğŸ”„ å¼€å§‹æ‰¹é‡æƒ…æ„Ÿåˆ†æ...");

    // ä½¿ç”¨æµå¼å¤„ç†æ‰¹é‡æ–‡æœ¬
    let results: Vec<_> = stream::iter(texts)
        .map(|text| {
            let client = client.clone();
            async move {
                let prompt = format!(
                    "è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼ˆç§¯æ/æ¶ˆæ/ä¸­æ€§ï¼‰ï¼Œå¹¶ç»™å‡ºç®€çŸ­è§£é‡Šï¼š\n\n\"{text}\""
                );
                let result = client.generate(&prompt).await;
                (text, result)
            }
        })
        .buffer_unordered(3) // é™åˆ¶å¹¶å‘æ•°ä¸º3
        .collect()
        .await;

    // æ˜¾ç¤ºç»“æœ
    for (text, result) in results {
        match result {
            Ok(analysis) => {
                println!("ğŸ“ æ–‡æœ¬: {text}");
                println!("ğŸ­ åˆ†æ: {analysis}\n");
            }
            Err(e) => {
                println!("âŒ å¤„ç†å¤±è´¥ '{text}': {e}\n");
            }
        }
    }

    println!("âœ… æ‰¹é‡å¤„ç†ç¤ºä¾‹å®Œæˆ\n");
    Ok(())
}

/// ç¤ºä¾‹3: æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ
async fn intelligent_chat_system(api_key: &str) -> Result<()> {
    println!("ğŸ§  ç¤ºä¾‹3: æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ");

    let config = Config::default()
        .with_api_key(api_key.to_string())
        .with_model("gpt-3.5-turbo".to_string());

    let client = LLMClient::new(config);

    // æ¨¡æ‹Ÿä¸€ä¸ªæ™ºèƒ½å®¢æœå¯¹è¯
    let system_prompt =
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯æ”¯æŒå®¢æœï¼Œå‹å¥½ã€è€å¿ƒã€ä¸“ä¸šã€‚ä½ éœ€è¦å¸®åŠ©ç”¨æˆ·è§£å†³æŠ€æœ¯é—®é¢˜ã€‚";

    // æ¨¡æ‹Ÿå¯¹è¯å†å²
    let mut conversation = vec![message("user", "ä½ å¥½ï¼Œæˆ‘çš„ç”µè„‘å¯åŠ¨å¾ˆæ…¢ï¼Œæ€ä¹ˆåŠï¼Ÿ")];

    println!("ğŸ’¬ å¼€å§‹æ™ºèƒ½å¯¹è¯...");

    for turn in 1..=3 {
        println!("\nğŸ”„ å¯¹è¯è½®æ¬¡ {turn}");

        // è·å–AIå›å¤
        let response = client
            .generate_with_context(system_prompt, &conversation)
            .await?;

        println!("ğŸ¤– å®¢æœ: {response}");

        // æ·»åŠ AIå›å¤åˆ°å¯¹è¯å†å²
        conversation.push(message("assistant", &response));

        // æ¨¡æ‹Ÿç”¨æˆ·çš„åç»­é—®é¢˜
        let user_followup = match turn {
            1 => "æˆ‘å·²ç»æ¸…ç†äº†ç£ç›˜ï¼Œè¿˜æœ‰å…¶ä»–å»ºè®®å—ï¼Ÿ",
            2 => "å¥½çš„ï¼Œæˆ‘ä¼šæ£€æŸ¥å¯åŠ¨é¡¹ã€‚è¿˜æœ‰ä»€ä¹ˆéœ€è¦æ³¨æ„çš„å—ï¼Ÿ",
            _ => "è°¢è°¢ä½ çš„å¸®åŠ©ï¼",
        };

        println!("ğŸ‘¤ ç”¨æˆ·: {user_followup}");
        conversation.push(message("user", user_followup));
    }

    println!("\nâœ… æ™ºèƒ½å¯¹è¯ç³»ç»Ÿç¤ºä¾‹å®Œæˆ\n");
    Ok(())
}

/// ç¤ºä¾‹4: æ€§èƒ½æµ‹è¯•å’Œç›‘æ§
async fn performance_monitoring_example(api_key: &str) -> Result<()> {
    println!("ğŸ“Š ç¤ºä¾‹4: æ€§èƒ½æµ‹è¯•å’Œç›‘æ§");

    let config = Config::default()
        .with_api_key(api_key.to_string())
        .with_model("gpt-3.5-turbo".to_string());

    let client = LLMClient::new(config);

    let test_prompt = "è¯·ç”¨ä¸€å¥è¯æè¿°äººå·¥æ™ºèƒ½ã€‚";
    let test_count = 5;

    println!("ğŸ”„ æ‰§è¡Œ {test_count} æ¬¡æ€§èƒ½æµ‹è¯•...");

    let mut response_times = Vec::new();
    let mut success_count = 0;

    for i in 1..=test_count {
        let start = Instant::now();

        match client.generate(test_prompt).await {
            Ok(response) => {
                let elapsed = start.elapsed();
                response_times.push(elapsed);
                success_count += 1;

                println!(
                    "âœ… æµ‹è¯• {}/{}: è€—æ—¶ {:?}, å“åº”é•¿åº¦: {} å­—ç¬¦",
                    i,
                    test_count,
                    elapsed,
                    response.len()
                );
            }
            Err(e) => {
                println!("âŒ æµ‹è¯• {i}/{test_count} å¤±è´¥: {e}");
            }
        }

        // é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        if i < test_count {
            sleep(Duration::from_millis(500)).await;
        }
    }

    // è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    if !response_times.is_empty() {
        let total_time: Duration = response_times.iter().sum();
        let avg_time = total_time / response_times.len() as u32;
        let min_time = response_times.iter().min().unwrap();
        let max_time = response_times.iter().max().unwrap();

        println!("\nğŸ“ˆ æ€§èƒ½ç»Ÿè®¡:");
        println!(
            "   æˆåŠŸç‡: {}/{} ({:.1}%)",
            success_count,
            test_count,
            (success_count as f64 / test_count as f64) * 100.0
        );
        println!("   å¹³å‡å“åº”æ—¶é—´: {avg_time:?}");
        println!("   æœ€å¿«å“åº”æ—¶é—´: {min_time:?}");
        println!("   æœ€æ…¢å“åº”æ—¶é—´: {max_time:?}");
    }

    println!("âœ… æ€§èƒ½ç›‘æ§ç¤ºä¾‹å®Œæˆ\n");
    Ok(())
}

/// ç¤ºä¾‹5: ä¸åŒæ¨¡å‹æ¯”è¾ƒ
async fn model_comparison_example(api_key: &str) -> Result<()> {
    println!("ğŸ”¬ ç¤ºä¾‹5: ä¸åŒæ¨¡å‹æ¯”è¾ƒ");

    let models = vec!["gpt-3.5-turbo", "gpt-4o-mini"];
    let test_prompt = "è¯·ç”¨åˆ›æ„çš„æ–¹å¼è§£é‡Šä»€ä¹ˆæ˜¯é€’å½’ã€‚";

    println!("ğŸ”„ ä½¿ç”¨ä¸åŒæ¨¡å‹ç”Ÿæˆå›ç­”...");

    for model in models {
        println!("\nğŸ¤– æ¨¡å‹: {model}");

        let config = Config::default()
            .with_api_key(api_key.to_string())
            .with_model(model.to_string())
            .with_temperature(0.8);

        let client = LLMClient::new(config);
        let start = Instant::now();

        match client.generate(test_prompt).await {
            Ok(response) => {
                let elapsed = start.elapsed();
                println!("â±ï¸ å“åº”æ—¶é—´: {elapsed:?}");
                println!("ğŸ“ å›ç­”: {response}");
            }
            Err(e) => {
                println!("âŒ æ¨¡å‹ {model} è°ƒç”¨å¤±è´¥: {e}");
            }
        }
    }

    println!("\nâœ… æ¨¡å‹æ¯”è¾ƒç¤ºä¾‹å®Œæˆ\n");
    Ok(())
}

/// è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºæµ‹è¯•é…ç½®
#[allow(dead_code)]
fn create_test_config(api_key: &str, model: &str) -> Config {
    Config::default()
        .with_api_key(api_key.to_string())
        .with_model(model.to_string())
        .with_temperature(0.7)
}

/// è¾…åŠ©å‡½æ•°ï¼šæ ¼å¼åŒ–å“åº”æ—¶é—´
#[allow(dead_code)]
fn format_duration(duration: Duration) -> String {
    if duration.as_secs() > 0 {
        format!("{:.2}s", duration.as_secs_f64())
    } else {
        format!("{}ms", duration.as_millis())
    }
}
