//! NanoAI æµå¼å¤„ç†ç¤ºä¾‹
//! ä¸“é—¨å±•ç¤ºæµå¼å“åº”çš„å„ç§ä½¿ç”¨åœºæ™¯

use futures::StreamExt;
use nanoai::{Config, LLMClient, Result, message};
use std::io::{self, Write};
use std::time::Instant;
use tokio::time::{Duration, sleep};

#[tokio::main]
async fn main() -> Result<()> {
    // åˆå§‹åŒ–æ—¥å¿—
    env_logger::init();

    // ä».envæ–‡ä»¶è·å–APIå¯†é’¥å’Œé…ç½®
    let (api_key, model) = if let Ok(key) = dotenvy::var("OPENROUTER_API_KEY") {
        let model = dotenvy::var("OPENROUTER_MODEL")
            .unwrap_or("tngtech/deepseek-r1t2-chimera:free".to_string());
        (key, model)
    } else if let Ok(key) = dotenvy::var("API_KEY") {
        (key, "tngtech/deepseek-r1t2-chimera:free".to_string())
    } else {
        println!("âŒ é”™è¯¯: æœªæ‰¾åˆ°OpenRouter APIå¯†é’¥");
        println!("\nè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€è®¾ç½®APIå¯†é’¥:");
        println!("\næ–¹å¼1: åˆ›å»º.envæ–‡ä»¶ (æ¨è)");
        println!("   OPENROUTER_API_KEY=your-openrouter-key");
        println!("   OPENROUTER_MODEL=your-model-name (å¯é€‰)");
        println!("\næ–¹å¼2: è®¾ç½®ç¯å¢ƒå˜é‡");
        println!("   Windows PowerShell: $env:OPENROUTER_API_KEY=\"your-api-key\"");
        println!("   Windows CMD: set OPENROUTER_API_KEY=your-api-key");
        return Ok(());
    };

    println!("âœ… APIå¯†é’¥å·²è®¾ç½®");
    println!("ğŸ”§ ä½¿ç”¨æ¨¡å‹: {model}");

    println!("ğŸŒŠ NanoAI æµå¼å¤„ç†ç¤ºä¾‹\n");

    // ç¤ºä¾‹1: åŸºç¡€æµå¼å“åº”
    basic_streaming_example(&api_key, &model).await?;

    // ç¤ºä¾‹2: å®æ—¶æ‰“å­—æ•ˆæœ
    typewriter_effect_example(&api_key, &model).await?;

    // ç¤ºä¾‹3: æµå¼å¯¹è¯
    streaming_conversation_example(&api_key, &model).await?;

    // ç¤ºä¾‹4: æµå¼å†…å®¹å¤„ç†
    stream_processing_example(&api_key, &model).await?;

    // ç¤ºä¾‹5: æµå¼é”™è¯¯å¤„ç†
    streaming_error_handling(&api_key, &model).await?;

    println!("\nâœ… æ‰€æœ‰æµå¼å¤„ç†ç¤ºä¾‹æ‰§è¡Œå®Œæˆï¼");
    Ok(())
}

/// ç¤ºä¾‹1: åŸºç¡€æµå¼å“åº”
async fn basic_streaming_example(api_key: &str, model: &str) -> Result<()> {
    println!("ğŸŒŠ ç¤ºä¾‹1: åŸºç¡€æµå¼å“åº”");

    let config = Config::default()
        .with_api_key(api_key.to_string())
        .with_model(model.to_string());

    let client = LLMClient::new(config);

    println!("ğŸ¤– AIæ­£åœ¨ç”Ÿæˆå›ç­”...");
    println!("å›ç­”: ");

    let mut stream = client
        .generate_stream("è¯·è¯¦ç»†è§£é‡Šä»€ä¹ˆæ˜¯Rustç¼–ç¨‹è¯­è¨€çš„æ‰€æœ‰æƒç³»ç»Ÿã€‚")
        .await?;

    let mut full_response = String::new();
    let start_time = Instant::now();

    while let Some(chunk_result) = stream.next().await {
        match chunk_result {
            Ok(chunk) => {
                print!("{chunk}");
                io::stdout().flush().unwrap();
                full_response.push_str(&chunk);
            }
            Err(e) => {
                eprintln!("\nâŒ æµå¼å“åº”é”™è¯¯: {e}");
                break;
            }
        }
    }

    let elapsed = start_time.elapsed();
    println!("\n\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:");
    println!("   æ€»å­—ç¬¦æ•°: {}", full_response.len());
    println!("   æ€»è€—æ—¶: {:?}", elapsed);
    println!(
        "   å¹³å‡é€Ÿåº¦: {:.1} å­—ç¬¦/ç§’",
        full_response.len() as f64 / elapsed.as_secs_f64()
    );

    println!("âœ… åŸºç¡€æµå¼å“åº”ç¤ºä¾‹å®Œæˆ\n");
    Ok(())
}

/// ç¤ºä¾‹2: å®æ—¶æ‰“å­—æ•ˆæœ
async fn typewriter_effect_example(api_key: &str, model: &str) -> Result<()> {
    println!("âŒ¨ï¸ ç¤ºä¾‹2: å®æ—¶æ‰“å­—æ•ˆæœ");

    let config = Config::default()
        .with_api_key(api_key.to_string())
        .with_model(model.to_string())
        .with_temperature(0.8);

    let client = LLMClient::new(config);

    println!("ğŸ¤– AIæ­£åœ¨åˆ›ä½œä¸€é¦–å…³äºç¼–ç¨‹çš„è¯—...");
    println!("\nğŸ“ è¯—æ­Œ:");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");

    let mut stream = client
        .generate_stream("è¯·å†™ä¸€é¦–å…³äºç¨‹åºå‘˜ç”Ÿæ´»çš„ç°ä»£è¯—ï¼Œè¦æœ‰èŠ‚å¥æ„Ÿå’ŒéŸµå¾‹ã€‚")
        .await?;

    while let Some(chunk_result) = stream.next().await {
        match chunk_result {
            Ok(chunk) => {
                // æ¨¡æ‹Ÿæ‰“å­—æœºæ•ˆæœ - é€å­—ç¬¦æ˜¾ç¤º
                for ch in chunk.chars() {
                    print!("{}", ch);
                    io::stdout().flush().unwrap();

                    // æ ¹æ®å­—ç¬¦ç±»å‹è°ƒæ•´å»¶è¿Ÿ
                    let delay = match ch {
                        'ã€‚' | 'ï¼' | 'ï¼Ÿ' => 200, // å¥å·åç¨é•¿åœé¡¿
                        'ï¼Œ' | 'ï¼›' => 100,        // é€—å·åçŸ­åœé¡¿
                        ' ' => 50,                 // ç©ºæ ¼åå¾ˆçŸ­åœé¡¿
                        _ => 30,                   // æ™®é€šå­—ç¬¦
                    };

                    sleep(Duration::from_millis(delay)).await;
                }
            }
            Err(e) => {
                eprintln!("\nâŒ æµå¼å“åº”é”™è¯¯: {e}");
                break;
            }
        }
    }

    println!("\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");
    println!("âœ… æ‰“å­—æ•ˆæœç¤ºä¾‹å®Œæˆ\n");
    Ok(())
}

/// ç¤ºä¾‹3: æµå¼å¯¹è¯
async fn streaming_conversation_example(api_key: &str, model: &str) -> Result<()> {
    println!("ğŸ’¬ ç¤ºä¾‹3: æµå¼å¯¹è¯");

    let config = Config::default()
        .with_api_key(api_key.to_string())
        .with_model(model.to_string());

    let client = LLMClient::new(config);

    let system_message = "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œå–œæ¬¢ç”¨è¡¨æƒ…ç¬¦å·ï¼Œå›ç­”è¦ç®€æ´æœ‰è¶£ã€‚";

    // æ¨¡æ‹Ÿå¤šè½®å¯¹è¯
    let conversations = [
        "ä½ å¥½ï¼ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        "æˆ‘æƒ³å­¦ä¹ ä¸€é—¨æ–°çš„ç¼–ç¨‹è¯­è¨€ï¼Œæœ‰ä»€ä¹ˆæ¨èå—ï¼Ÿ",
        "Rustè¯­è¨€æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
    ];

    let mut message_history = Vec::new();

    for (i, user_input) in conversations.iter().enumerate() {
        println!("\nğŸ”„ å¯¹è¯è½®æ¬¡ {}", i + 1);
        println!("ğŸ‘¤ ç”¨æˆ·: {user_input}");

        // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        message_history.push(message("user", user_input));

        print!("ğŸ¤– AI: ");
        io::stdout().flush().unwrap();

        // æµå¼ç”Ÿæˆå›å¤
        let mut stream = client
            .generate_stream_with_context(system_message, &message_history)
            .await?;

        let mut ai_response = String::new();

        while let Some(chunk_result) = stream.next().await {
            match chunk_result {
                Ok(chunk) => {
                    print!("{}", chunk);
                    io::stdout().flush().unwrap();
                    ai_response.push_str(&chunk);
                }
                Err(e) => {
                    eprintln!("\nâŒ æµå¼å“åº”é”™è¯¯: {e}");
                    break;
                }
            }
        }

        // æ·»åŠ AIå›å¤åˆ°å†å²
        message_history.push(message("assistant", &ai_response));

        println!(); // æ¢è¡Œ
    }

    println!("âœ… æµå¼å¯¹è¯ç¤ºä¾‹å®Œæˆ\n");
    Ok(())
}

/// ç¤ºä¾‹4: æµå¼å†…å®¹å¤„ç†
async fn stream_processing_example(api_key: &str, model: &str) -> Result<()> {
    println!("ğŸ”„ ç¤ºä¾‹4: æµå¼å†…å®¹å¤„ç†");

    let config = Config::default()
        .with_api_key(api_key.to_string())
        .with_model(model.to_string());

    let client = LLMClient::new(config);

    println!("ğŸ¤– AIæ­£åœ¨ç”ŸæˆæŠ€æœ¯æ–‡ç« ...");

    let mut stream = client
        .generate_stream("è¯·å†™ä¸€ç¯‡å…³äº'å¦‚ä½•ä¼˜åŒ–Rustç¨‹åºæ€§èƒ½'çš„æŠ€æœ¯æ–‡ç« ï¼ŒåŒ…å«å…·ä½“çš„ä»£ç ç¤ºä¾‹ã€‚")
        .await?;

    let mut word_count = 0;
    let mut sentence_count = 0;
    let mut paragraph_count = 0;
    let mut current_word = String::new();
    let mut buffer = String::new();

    println!("\nğŸ“„ æ–‡ç« å†…å®¹:");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    while let Some(chunk_result) = stream.next().await {
        match chunk_result {
            Ok(chunk) => {
                print!("{}", chunk);
                io::stdout().flush().unwrap();

                // å®æ—¶ç»Ÿè®¡
                for ch in chunk.chars() {
                    buffer.push(ch);

                    match ch {
                        ' ' | '\n' | '\t' => {
                            if !current_word.is_empty() {
                                word_count += 1;
                                current_word.clear();
                            }
                        }
                        'ã€‚' | 'ï¼' | 'ï¼Ÿ' => {
                            sentence_count += 1;
                        }
                        _ => {
                            current_word.push(ch);
                        }
                    }
                }

                // æ£€æµ‹æ®µè½
                if chunk.contains("\n\n") {
                    paragraph_count += chunk.matches("\n\n").count();
                }
            }
            Err(e) => {
                eprintln!("\nâŒ æµå¼å“åº”é”™è¯¯: {e}");
                break;
            }
        }
    }

    // å¤„ç†æœ€åä¸€ä¸ªè¯
    if !current_word.is_empty() {
        word_count += 1;
    }

    println!("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("ğŸ“Š å®æ—¶ç»Ÿè®¡ç»“æœ:");
    println!("   å­—ç¬¦æ•°: {}", buffer.len());
    println!("   è¯æ•°: {word_count}");
    println!("   å¥å­æ•°: {}", sentence_count);
    println!("   æ®µè½æ•°: {}", paragraph_count.max(1));

    println!("âœ… æµå¼å†…å®¹å¤„ç†ç¤ºä¾‹å®Œæˆ\n");
    Ok(())
}

/// ç¤ºä¾‹5: æµå¼é”™è¯¯å¤„ç†
async fn streaming_error_handling(api_key: &str, model: &str) -> Result<()> {
    println!("ğŸ›¡ï¸ ç¤ºä¾‹5: æµå¼é”™è¯¯å¤„ç†");

    // é¦–å…ˆæ¼”ç¤ºæ­£å¸¸çš„æµå¼å¤„ç†
    println!("ğŸ”„ æ­£å¸¸æµå¼å¤„ç†:");
    let config = Config::default()
        .with_api_key(api_key.to_string())
        .with_model(model.to_string());

    let client = LLMClient::new(config);

    let mut stream = client.generate_stream("ç®€å•ä»‹ç»ä¸€ä¸‹Rustè¯­è¨€ã€‚").await?;

    let mut chunk_count = 0;
    let mut error_count = 0;

    print!("ğŸ¤– å›ç­”: ");

    while let Some(chunk_result) = stream.next().await {
        match chunk_result {
            Ok(chunk) => {
                print!("{}", chunk);
                io::stdout().flush().unwrap();
                chunk_count += 1;
            }
            Err(e) => {
                error_count += 1;
                eprintln!("\nâš ï¸ å¤„ç†ç¬¬ {} ä¸ªæ•°æ®å—æ—¶å‡ºé”™: {}", chunk_count + 1, e);

                // æ ¹æ®é”™è¯¯ç±»å‹å†³å®šæ˜¯å¦ç»§ç»­
                match e {
                    nanoai::NanoError::StreamError(_) => {
                        println!("ğŸ”„ å°è¯•ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ•°æ®å—...");
                        continue;
                    }
                    _ => {
                        println!("âŒ ä¸¥é‡é”™è¯¯ï¼Œåœæ­¢å¤„ç†");
                        break;
                    }
                }
            }
        }
    }

    println!("\n\nğŸ“Š å¤„ç†ç»Ÿè®¡:");
    println!("   æˆåŠŸå¤„ç†çš„æ•°æ®å—: {}", chunk_count);
    println!("   é”™è¯¯æ•°é‡: {}", error_count);

    // æ¼”ç¤ºé”™è¯¯é…ç½®çš„å¤„ç†
    println!("\nğŸ”„ é”™è¯¯é…ç½®æ¼”ç¤º:");
    let bad_config = Config::default()
        .with_api_key("invalid_key".to_string())
        .with_model(model.to_string());

    let bad_client = LLMClient::new(bad_config);

    match bad_client.generate_stream("Hello").await {
        Ok(mut stream) => {
            println!("ğŸ”„ å¼€å§‹å¤„ç†æµ...");
            while let Some(chunk_result) = stream.next().await {
                match chunk_result {
                    Ok(chunk) => {
                        print!("{chunk}");
                    }
                    Err(e) => {
                        println!("âŒ é¢„æœŸçš„æµå¼é”™è¯¯: {e}");
                        break;
                    }
                }
            }
        }
        Err(e) => {
            println!("âŒ é¢„æœŸçš„åˆå§‹åŒ–é”™è¯¯: {e}");
        }
    }

    println!("âœ… æµå¼é”™è¯¯å¤„ç†ç¤ºä¾‹å®Œæˆ\n");
    Ok(())
}
