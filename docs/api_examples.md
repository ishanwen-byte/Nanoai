# NanoAI API è°ƒç”¨è¯¦ç»†ç¤ºä¾‹

æœ¬æ–‡æ¡£æä¾›äº† NanoAI åº“æ‰€æœ‰ API æ–¹æ³•çš„è¯¦ç»†ä½¿ç”¨ç¤ºä¾‹å’Œæœ€ä½³å®è·µã€‚

## ğŸ“š ç›®å½•

- [åŸºç¡€é…ç½®](#åŸºç¡€é…ç½®)
- [æ ¸å¿ƒ API æ–¹æ³•](#æ ¸å¿ƒ-api-æ–¹æ³•)
- [é«˜çº§ç”¨æ³•](#é«˜çº§ç”¨æ³•)
- [é”™è¯¯å¤„ç†](#é”™è¯¯å¤„ç†)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

## åŸºç¡€é…ç½®

### 1. é»˜è®¤é…ç½®

```rust
use nanoai::{Config, LLMClient};

// ä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡åŠ è½½ï¼‰
let config = Config::from_env();
let client = LLMClient::new(config);
```

### 2. è‡ªå®šä¹‰é…ç½®

```rust
use nanoai::Config;

// æ„å»ºå™¨æ¨¡å¼é…ç½®
let config = Config::builder()
    .api_key("your-api-key")
    .model("gpt-3.5-turbo")
    .base_url("https://api.openai.com/v1")
    .temperature(0.7)
    .max_tokens(1000)
    .timeout(30)
    .with_random_seed_auto()  // è‡ªåŠ¨ç”Ÿæˆéšæœºç§å­
    .build();

let client = LLMClient::new(config);
```

### 3. ç¯å¢ƒå˜é‡é…ç½®

```bash
# .env æ–‡ä»¶
OPENAI_API_KEY=your-api-key
OPENAI_MODEL=gpt-3.5-turbo
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=1000
OPENAI_TIMEOUT=30
```

## æ ¸å¿ƒ API æ–¹æ³•

### 1. `generate()` - åŸºç¡€æ–‡æœ¬ç”Ÿæˆ

**æ–¹æ³•ç­¾åï¼š**
```rust
pub async fn generate(&self, prompt: &str) -> Result<String, NanoError>
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
```rust
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = Config::from_env();
    let client = LLMClient::new(config);
    
    // ç®€å•çš„æ–‡æœ¬ç”Ÿæˆ
    let response = client.generate("è¯·è§£é‡Šä»€ä¹ˆæ˜¯ Rust ç¼–ç¨‹è¯­è¨€ï¼Ÿ").await?;
    println!("AI å›å¤: {}", response);
    
    Ok(())
}
```

**é€‚ç”¨åœºæ™¯ï¼š**
- ç®€å•çš„é—®ç­”
- æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
- ä¸éœ€è¦ç»Ÿè®¡ä¿¡æ¯çš„åœºæ™¯

### 2. `generate_with_stats()` - å¸¦ç»Ÿè®¡ä¿¡æ¯çš„æ–‡æœ¬ç”Ÿæˆ

**æ–¹æ³•ç­¾åï¼š**
```rust
pub async fn generate_with_stats(&self, prompt: &str) -> Result<ResponseWithStats, NanoError>
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
```rust
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = Config::from_env();
    let client = LLMClient::new(config);
    
    let response = client.generate_with_stats("è¯·å†™ä¸€ä¸ª Rust å‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—").await?;
    
    println!("AI å›å¤: {}", response.content);
    println!("ç»Ÿè®¡ä¿¡æ¯:");
    println!("  - ç”¨æ—¶: {}ms", response.stats.duration_ms);
    println!("  - è¾“å…¥ tokens: {}", response.stats.prompt_tokens.unwrap_or(0));
    println!("  - è¾“å‡º tokens: {}", response.stats.completion_tokens.unwrap_or(0));
    println!("  - æ€» tokens: {}", response.stats.total_tokens.unwrap_or(0));
    
    Ok(())
}
```

**é€‚ç”¨åœºæ™¯ï¼š**
- éœ€è¦ç›‘æ§ API ä½¿ç”¨æƒ…å†µ
- æ€§èƒ½åˆ†æå’Œä¼˜åŒ–
- æˆæœ¬æ§åˆ¶å’Œé¢„ç®—ç®¡ç†

### 3. `generate_with_context_stats()` - å¤šè½®å¯¹è¯

**æ–¹æ³•ç­¾åï¼š**
```rust
pub async fn generate_with_context_stats(
    &self,
    system_prompt: &str,
    messages: &[Message],
) -> Result<ResponseWithStats, NanoError>
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
```rust
use nanoai::{Config, LLMClient, message};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = Config::from_env();
    let client = LLMClient::new(config);
    
    // æ„å»ºå¯¹è¯å†å²
    let messages = vec![
        message("user", "æˆ‘æƒ³å­¦ä¹  Rust ç¼–ç¨‹"),
        message("assistant", "å¾ˆå¥½çš„é€‰æ‹©ï¼Rust æ˜¯ä¸€é—¨ç³»ç»Ÿç¼–ç¨‹è¯­è¨€ï¼Œä»¥å†…å­˜å®‰å…¨å’Œé«˜æ€§èƒ½è‘—ç§°ã€‚"),
        message("user", "è¯·æ¨èä¸€äº›å­¦ä¹ èµ„æº"),
    ];
    
    let response = client.generate_with_context_stats(
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹å¯¼å¸ˆï¼Œè¯·æä¾›å‡†ç¡®ã€å®ç”¨çš„å»ºè®®ã€‚",
        &messages
    ).await?;
    
    println!("AI å›å¤: {}", response.content);
    println!("å¯¹è¯ç»Ÿè®¡: {}ms, {} tokens", 
             response.stats.duration_ms,
             response.stats.total_tokens.unwrap_or(0));
    
    Ok(())
}
```

**é€‚ç”¨åœºæ™¯ï¼š**
- èŠå¤©æœºå™¨äºº
- å¤šè½®å¯¹è¯ç³»ç»Ÿ
- ä¸Šä¸‹æ–‡ç›¸å…³çš„é—®ç­”

### 4. `generate_stream()` - æµå¼å“åº”

**æ–¹æ³•ç­¾åï¼š**
```rust
pub async fn generate_stream(
    &self,
    prompt: &str,
) -> Result<impl Stream<Item = Result<String, NanoError>>, NanoError>
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
```rust
use futures::StreamExt;
use std::io::{self, Write};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = Config::from_env();
    let client = LLMClient::new(config);
    
    let mut stream = client.generate_stream("è¯·å†™ä¸€ä¸ªå…³äº Rust çš„æŠ€æœ¯åšå®¢æ–‡ç« ").await?;
    
    print!("AI æ­£åœ¨ç”Ÿæˆå†…å®¹: ");
    io::stdout().flush().unwrap();
    
    while let Some(result) = stream.next().await {
        match result {
            Ok(content) => {
                print!("{}", content);
                io::stdout().flush().unwrap();
            }
            Err(e) => {
                eprintln!("\næµå¼é”™è¯¯: {:?}", e);
                break;
            }
        }
    }
    
    println!("\n\nç”Ÿæˆå®Œæˆï¼");
    Ok(())
}
```

**é€‚ç”¨åœºæ™¯ï¼š**
- å®æ—¶æ‰“å­—æ•ˆæœ
- é•¿æ–‡æœ¬ç”Ÿæˆ
- ç”¨æˆ·ä½“éªŒä¼˜åŒ–

## é«˜çº§ç”¨æ³•

### 1. å¹¶å‘å¤„ç†å¤šä¸ªè¯·æ±‚

```rust
use futures::future::join_all;
use tokio;
use std::time::Instant;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = Config::from_env();
    let client = LLMClient::new(config);
    
    let questions = vec![
        "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "Rust çš„æ‰€æœ‰æƒç³»ç»Ÿæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯å‡½æ•°å¼ç¼–ç¨‹çš„ä¼˜åŠ¿ï¼Ÿ",
        "è¯·ä»‹ç»ä¸€ä¸‹åŒºå—é“¾æŠ€æœ¯",
        "å¦‚ä½•ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ï¼Ÿ",
    ];
    
    let start_time = Instant::now();
    
    // åˆ›å»ºå¹¶å‘ä»»åŠ¡
    let tasks: Vec<_> = questions.into_iter().enumerate().map(|(i, question)| {
        let client = client.clone();
        tokio::spawn(async move {
            let task_start = Instant::now();
            let result = client.generate_with_stats(question).await;
            let task_duration = task_start.elapsed();
            (i, question, result, task_duration)
        })
    }).collect();
    
    // ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    let results = join_all(tasks).await;
    let total_duration = start_time.elapsed();
    
    // ç»Ÿè®¡ä¿¡æ¯
    let mut successful_requests = 0;
    let mut failed_requests = 0;
    let mut total_tokens = 0;
    
    // å¤„ç†ç»“æœ
    for task_result in results {
        match task_result {
            Ok((index, question, Ok(response), task_duration)) => {
                successful_requests += 1;
                total_tokens += response.stats.total_tokens.unwrap_or(0);
                
                println!("\n=== é—®é¢˜ {} ===", index + 1);
                println!("é—®é¢˜: {}", question);
                println!("å›ç­”: {}...", 
                         response.content.chars().take(100).collect::<String>());
                println!("ç»Ÿè®¡: {}ms, {} tokens", 
                         response.stats.duration_ms,
                         response.stats.total_tokens.unwrap_or(0));
                println!("ä»»åŠ¡ç”¨æ—¶: {:?}", task_duration);
            }
            Ok((index, _question, Err(e), _)) => {
                failed_requests += 1;
                println!("é—®é¢˜ {} å¤±è´¥: {}", index + 1, e);
            }
            Err(e) => {
                failed_requests += 1;
                println!("ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {}", e);
            }
        }
    }
    
    // æœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š
    println!("\n=== å¹¶å‘å¤„ç†ç»Ÿè®¡æŠ¥å‘Š ===");
    println!("æ€»ç”¨æ—¶: {:?}", total_duration);
    println!("æˆåŠŸè¯·æ±‚: {}", successful_requests);
    println!("å¤±è´¥è¯·æ±‚: {}", failed_requests);
    println!("æ€» tokens: {}", total_tokens);
    println!("å¹³å‡æ¯ä¸ªè¯·æ±‚ç”¨æ—¶: {:?}", 
             total_duration / (successful_requests + failed_requests) as u32);
    
    Ok(())
}
```

### 2. æ™ºèƒ½é‡è¯•æœºåˆ¶

```rust
use std::time::Duration;
use tokio::time::sleep;

async fn generate_with_retry(
    client: &LLMClient,
    prompt: &str,
    max_retries: u32,
) -> Result<ResponseWithStats, NanoError> {
    let mut last_error = None;
    
    for attempt in 0..=max_retries {
        match client.generate_with_stats(prompt).await {
            Ok(response) => {
                if attempt > 0 {
                    println!("é‡è¯•æˆåŠŸï¼Œç¬¬ {} æ¬¡å°è¯•", attempt + 1);
                }
                return Ok(response);
            }
            Err(e) => {
                last_error = Some(e);
                if attempt < max_retries {
                    let delay = Duration::from_millis(1000 * (2_u64.pow(attempt)));
                    println!("è¯·æ±‚å¤±è´¥ï¼Œ{}ms åé‡è¯•...", delay.as_millis());
                    sleep(delay).await;
                }
            }
        }
    }
    
    Err(last_error.unwrap())
}

// ä½¿ç”¨ç¤ºä¾‹
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = Config::from_env();
    let client = LLMClient::new(config);
    
    let response = generate_with_retry(
        &client,
        "è¯·è§£é‡Šé‡å­è®¡ç®—çš„åŸºæœ¬åŸç†",
        3  // æœ€å¤šé‡è¯• 3 æ¬¡
    ).await?;
    
    println!("æœ€ç»ˆå›å¤: {}", response.content);
    Ok(())
}
```

## é”™è¯¯å¤„ç†

### é”™è¯¯ç±»å‹è¯´æ˜

```rust
use nanoai::NanoError;

#[tokio::main]
async fn main() {
    let config = Config::from_env();
    let client = LLMClient::new(config);
    
    match client.generate("æµ‹è¯•é—®é¢˜").await {
        Ok(response) => {
            println!("æˆåŠŸ: {}", response);
        }
        Err(e) => {
            match e {
                NanoError::ConfigError(msg) => {
                    eprintln!("é…ç½®é”™è¯¯: {}", msg);
                    // æ£€æŸ¥ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶
                }
                NanoError::NetworkError(msg) => {
                    eprintln!("ç½‘ç»œé”™è¯¯: {}", msg);
                    // æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ä»£ç†è®¾ç½®
                }
                NanoError::ApiError { code, message } => {
                    eprintln!("API é”™è¯¯ {}: {}", code, message);
                    // æ£€æŸ¥ API å¯†é’¥æˆ–è¯·æ±‚å‚æ•°
                }
                NanoError::ParseError(msg) => {
                    eprintln!("è§£æé”™è¯¯: {}", msg);
                    // API å“åº”æ ¼å¼å¼‚å¸¸
                }
                NanoError::TimeoutError => {
                    eprintln!("è¯·æ±‚è¶…æ—¶");
                    // å¢åŠ è¶…æ—¶æ—¶é—´æˆ–æ£€æŸ¥ç½‘ç»œ
                }
                NanoError::RateLimitError => {
                    eprintln!("è¯·æ±‚é¢‘ç‡é™åˆ¶");
                    // å®ç°é€€é¿é‡è¯•ç­–ç•¥
                }
            }
        }
    }
}
```

## æ€§èƒ½ä¼˜åŒ–

### 1. è¿æ¥æ± å’Œå®¢æˆ·ç«¯å¤ç”¨

```rust
use std::sync::Arc;
use tokio::sync::Mutex;

// å…¨å±€å®¢æˆ·ç«¯å®ä¾‹
static CLIENT: once_cell::sync::Lazy<Arc<LLMClient>> = once_cell::sync::Lazy::new(|| {
    let config = Config::from_env();
    Arc::new(LLMClient::new(config))
});

// åœ¨å¤šä¸ªåœ°æ–¹å¤ç”¨å®¢æˆ·ç«¯
async fn process_request(prompt: &str) -> Result<String, NanoError> {
    CLIENT.generate(prompt).await
}
```

### 2. æ‰¹é‡å¤„ç†ä¼˜åŒ–

```rust
use futures::stream::{self, StreamExt};

async fn batch_process(
    client: &LLMClient,
    prompts: Vec<String>,
    concurrency_limit: usize,
) -> Vec<Result<ResponseWithStats, NanoError>> {
    stream::iter(prompts)
        .map(|prompt| async move {
            client.generate_with_stats(&prompt).await
        })
        .buffer_unordered(concurrency_limit)
        .collect()
        .await
}
```

## æœ€ä½³å®è·µ

### 1. é…ç½®ç®¡ç†

```rust
// æ¨èï¼šä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®
let config = Config::from_env();

// å¼€å‘ç¯å¢ƒï¼šä½¿ç”¨ .env æ–‡ä»¶
// ç”Ÿäº§ç¯å¢ƒï¼šä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡æˆ–é…ç½®ç®¡ç†æœåŠ¡
```

### 2. é”™è¯¯å¤„ç†ç­–ç•¥

```rust
// æ¨èï¼šè¯¦ç»†çš„é”™è¯¯å¤„ç†
match client.generate(prompt).await {
    Ok(response) => { /* å¤„ç†æˆåŠŸå“åº” */ }
    Err(NanoError::RateLimitError) => {
        // å®ç°é€€é¿é‡è¯•
        tokio::time::sleep(Duration::from_secs(1)).await;
        // é‡è¯•é€»è¾‘
    }
    Err(e) => {
        // è®°å½•é”™è¯¯æ—¥å¿—
        log::error!("AI è¯·æ±‚å¤±è´¥: {:?}", e);
        // è¿”å›ç”¨æˆ·å‹å¥½çš„é”™è¯¯ä¿¡æ¯
    }
}
```

### 3. èµ„æºç®¡ç†

```rust
// æ¨èï¼šåˆç†è®¾ç½®è¶…æ—¶å’Œé™åˆ¶
let config = Config::builder()
    .timeout(30)  // 30ç§’è¶…æ—¶
    .max_tokens(1000)  // é™åˆ¶è¾“å‡ºé•¿åº¦
    .temperature(0.7)  // å¹³è¡¡åˆ›é€ æ€§å’Œå‡†ç¡®æ€§
    .build();
```

### 4. ç›‘æ§å’Œæ—¥å¿—

```rust
use log::{info, warn, error};

let start = std::time::Instant::now();
let response = client.generate_with_stats(prompt).await?;
let duration = start.elapsed();

info!("AI è¯·æ±‚å®Œæˆ: {}ms, {} tokens", 
      duration.as_millis(),
      response.stats.total_tokens.unwrap_or(0));

if duration.as_secs() > 10 {
    warn!("AI è¯·æ±‚è€—æ—¶è¿‡é•¿: {:?}", duration);
}
```

---

## æ€»ç»“

NanoAI æä¾›äº†ç®€æ´è€Œå¼ºå¤§çš„ APIï¼Œæ”¯æŒä»ç®€å•çš„æ–‡æœ¬ç”Ÿæˆåˆ°å¤æ‚çš„å¹¶å‘å¤„ç†åœºæ™¯ã€‚é€šè¿‡åˆç†ä½¿ç”¨è¿™äº› API å’Œæœ€ä½³å®è·µï¼Œå¯ä»¥æ„å»ºé«˜æ•ˆã€å¯é çš„ AI åº”ç”¨ç¨‹åºã€‚

å…³é”®è¦ç‚¹ï¼š
- ä½¿ç”¨ `generate_with_stats()` è¿›è¡Œæ€§èƒ½ç›‘æ§
- åˆ©ç”¨ `generate_with_context_stats()` å®ç°å¤šè½®å¯¹è¯
- é€šè¿‡ `generate_stream()` æå‡ç”¨æˆ·ä½“éªŒ
- å®ç°é€‚å½“çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
- åˆç†é…ç½®å¹¶å‘é™åˆ¶å’Œè¶…æ—¶å‚æ•°